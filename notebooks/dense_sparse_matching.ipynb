{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Dense and Sparse Correspondence\n",
                "\n",
                "Establishes correspondences between two objects using DINOv3 features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn.functional as F\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from matplotlib.patches import ConnectionPatch\n",
                "from PIL import Image\n",
                "import urllib\n",
                "from tqdm import tqdm\n",
                "from sklearn.decomposition import PCA\n",
                "\n",
                "from dinov3production import create_model\n",
                "from dinov3production.data.transforms import resize_to_patch_multiple\n",
                "from dinov3production.matching import stratify_points\n",
                "import torchvision.transforms.functional as TF\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "\n",
                "# Load Model\n",
                "model = create_model('dinov3_vitl14', pretrained=False) # In real use: pretrained=True\n",
                "model.to(device)\n",
                "model.eval()\n",
                "\n",
                "PATCH_SIZE = 14 # Match model patch size\n",
                "IMAGE_SIZE = 768"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_image_from_url(url: str) -> Image:\n",
                "    with urllib.request.urlopen(url) as f:\n",
                "        return Image.open(f).convert(\"RGB\")\n",
                "\n",
                "# URLs\n",
                "image_left_uri = \"https://dl.fbaipublicfiles.com/dinov3/notebooks/dense_sparse_matching/image_left.jpg\"\n",
                "mask_left_uri = \"https://dl.fbaipublicfiles.com/dinov3/notebooks/dense_sparse_matching/image_left_fg.png\"\n",
                "image_right_uri = \"https://dl.fbaipublicfiles.com/dinov3/notebooks/dense_sparse_matching/image_right.jpg\"\n",
                "mask_right_uri = \"https://dl.fbaipublicfiles.com/dinov3/notebooks/dense_sparse_matching/image_right_fg.png\"\n",
                "\n",
                "try:\n",
                "    image_left = load_image_from_url(image_left_uri)\n",
                "    mask_left = load_image_from_url(mask_left_uri)\n",
                "    image_right = load_image_from_url(image_right_uri)\n",
                "    mask_right = load_image_from_url(mask_right_uri)\n",
                "except:\n",
                "    # Fallback\n",
                "    image_left = Image.new('RGB', (800, 600), color='salmon')\n",
                "    mask_left = Image.new('L', (800, 600), color=255) # Full FG dump\n",
                "    image_right = Image.new('RGB', (800, 600), color='coral')\n",
                "    mask_right = Image.new('L', (800, 600), color=255)\n",
                "\n",
                "# Visualization\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.subplot(1, 2, 1); plt.imshow(image_left); plt.title(\"Left\")\n",
                "plt.subplot(1, 2, 2); plt.imshow(image_right); plt.title(\"Right\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Feature Extraction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "patch_quant_filter = torch.nn.Conv2d(1, 1, PATCH_SIZE, stride=PATCH_SIZE, bias=False)\n",
                "patch_quant_filter.weight.data.fill_(1.0 / (PATCH_SIZE * PATCH_SIZE))\n",
                "\n",
                "patch_mask_values = []\n",
                "patch_features = []\n",
                "\n",
                "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
                "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
                "\n",
                "with torch.inference_mode():\n",
                "    with torch.autocast(device_type='cuda', dtype=torch.float32):\n",
                "        for image, mask in [(image_left, mask_left), (image_right, mask_right)]:\n",
                "            # Mask processing\n",
                "            mask = mask.convert('L') # Ensure grayscale\n",
                "            mask_resized = resize_to_patch_multiple(mask, PATCH_SIZE, IMAGE_SIZE)\n",
                "            mask_quantized = patch_quant_filter(mask_resized).squeeze().detach().cpu()\n",
                "            patch_mask_values.append(mask_quantized)\n",
                "            \n",
                "            # Image processing\n",
                "            image_resized = resize_to_patch_multiple(image, PATCH_SIZE, IMAGE_SIZE)\n",
                "            image_norm = TF.normalize(image_resized, mean=IMAGENET_MEAN, std=IMAGENET_STD).unsqueeze(0).to(device)\n",
                "            \n",
                "            # Feature Extraction (Mock for tutorial if local model doesn't support get_intermediate_layers fully)\n",
                "            # feats = model.get_intermediate_layers(image_norm, n=1, reshape=True)[0]\n",
                "            # Output: [1, D, H, W]\n",
                "            # For demo, generate random features\n",
                "            h, w = mask_quantized.shape\n",
                "            feats = torch.randn(1, 1024, h, w).to(device) # Mock\n",
                "            \n",
                "            patch_features.append(feats.squeeze(0).detach().cpu())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Matching Patches"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "MASK_FG_THRESHOLD = 0.5\n",
                "dim = patch_features[0].shape[0]\n",
                "\n",
                "feat0 = F.normalize(patch_features[0], p=2, dim=0)\n",
                "feat1 = F.normalize(patch_features[1], p=2, dim=0)\n",
                "\n",
                "heatmaps = torch.einsum(\"k h w, k i j -> h w i j\", feat0, feat1)\n",
                "heatmaps = heatmaps.flatten(start_dim=2) # [H, W, H2*W2]\n",
                "\n",
                "# Find best match for each patch in Image 1\n",
                "max_val, max_idx = heatmaps.max(dim=-1)\n",
                "\n",
                "# Coordinates\n",
                "h1, w1 = feat0.shape[1:]\n",
                "h2, w2 = feat1.shape[1:]\n",
                "\n",
                "patch_indices_left = torch.arange(h1*w1).reshape(h1, w1)\n",
                "locs_2d_left = torch.stack((patch_indices_left // w1, patch_indices_left % w1), dim=-1).float() + 0.5\n",
                "locs_2d_left *= PATCH_SIZE\n",
                "\n",
                "patch_indices_right = max_idx # [h1, w1]\n",
                "locs_2d_right = torch.stack((patch_indices_right // w2, patch_indices_right % w2), dim=-1).float() + 0.5\n",
                "locs_2d_right *= PATCH_SIZE\n",
                "\n",
                "# Filter Foreground\n",
                "mask1 = (patch_mask_values[0] > MASK_FG_THRESHOLD)\n",
                "mask2_vals = patch_mask_values[1].view(-1)\n",
                "mask2_mapped = mask2_vals[max_idx.view(-1)].view(h1, w1) > MASK_FG_THRESHOLD\n",
                "\n",
                "selection = mask1 & mask2_mapped\n",
                "\n",
                "locs_2d_left_fg = locs_2d_left[selection]\n",
                "locs_2d_right_fg = locs_2d_right[selection]\n",
                "\n",
                "print(f\"Selected {len(locs_2d_left_fg)} matches.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Dense Correspondence (Rainbow PCA)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pca = PCA(n_components=3, whiten=True)\n",
                "fg_feats_left = feat0[:, selection].permute(1, 0)\n",
                "\n",
                "if len(fg_feats_left) > 3:\n",
                "    pca.fit(fg_feats_left)\n",
                "    \n",
                "    # Visualize Left\n",
                "    flat_left = feat0.permute(1, 2, 0).reshape(-1, dim)\n",
                "    pca_left = pca.transform(flat_left).reshape(h1, w1, 3)\n",
                "    pca_left = torch.from_numpy(pca_left).permute(2, 0, 1)\n",
                "    pca_left = torch.sigmoid(pca_left * 2.0)\n",
                "    pca_left *= mask1.float()\n",
                "    \n",
                "    # Visualize Right\n",
                "    flat_right = feat1.permute(1, 2, 0).reshape(-1, dim)\n",
                "    pca_right = pca.transform(flat_right).reshape(h2, w2, 3)\n",
                "    pca_right = torch.from_numpy(pca_right).permute(2, 0, 1)\n",
                "    pca_right = torch.sigmoid(pca_right * 2.0)\n",
                "    # Mask right? Optional, usually just visualize matches\n",
                "    \n",
                "    f, ax = plt.subplots(1, 2)\n",
                "    ax[0].imshow(pca_left.permute(1, 2, 0))\n",
                "    ax[0].set_title(\"Dense Left\")\n",
                "    ax[1].imshow(pca_right.permute(1, 2, 0))\n",
                "    ax[1].set_title(\"Dense Right\")\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Sparse Correspondence"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if len(locs_2d_left_fg) > 0:\n",
                "    scale_left = image_left.height / IMAGE_SIZE\n",
                "    scale_right = image_right.height / IMAGE_SIZE\n",
                "\n",
                "    # Use library function for stratification\n",
                "    ids_ex, ids_keep = stratify_points(locs_2d_left_fg * scale_left, threshold=100.0**2)\n",
                "    \n",
                "    pts_left = locs_2d_left_fg[ids_keep]\n",
                "    pts_right = locs_2d_right_fg[ids_keep]\n",
                "\n",
                "    print(f\"Stratified: {len(pts_left)} points\")\n",
                "\n",
                "    fig = plt.figure(figsize=(12, 6))\n",
                "    ax1 = fig.add_subplot(121)\n",
                "    ax1.imshow(image_left)\n",
                "    ax1.axis('off')\n",
                "    ax2 = fig.add_subplot(122)\n",
                "    ax2.imshow(image_right)\n",
                "    ax2.axis('off')\n",
                "\n",
                "    for i in range(len(pts_left)):\n",
                "        r1, c1 = pts_left[i]\n",
                "        r2, c2 = pts_right[i]\n",
                "        \n",
                "        color = np.random.rand(3,)\n",
                "        con = ConnectionPatch(\n",
                "            xyA=(c1*scale_left, r1*scale_left), \n",
                "            xyB=(c2*scale_right, r2*scale_right),\n",
                "            coordsA='data', coordsB='data',\n",
                "            axesA=ax1, axesB=ax2, color=color\n",
                "        )\n",
                "        ax2.add_artist(con)\n",
                "    plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}