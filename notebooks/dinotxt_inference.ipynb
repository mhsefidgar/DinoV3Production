{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DINO-Text Inference\n",
                "\n",
                "Open-vocabulary classification using DINOv3 features and text encoders.\n",
                "Demonstrates zero-shot classification, patch alignment, and ImageNet evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn.functional as F\n",
                "import urllib\n",
                "from PIL import Image\n",
                "import sys\n",
                "import numpy as np\n",
                "\n",
                "# Add path if running locally from repo root\n",
                "sys.path.append(\"../\")\n",
                "\n",
                "from dinov3production.hub.dinotxt import dinov3_vitl16_dinotxt_tet1280d20h24l\n",
                "from dinov3production.data.transforms import make_classification_eval_transform\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Model\n",
                "Load the DINOv3-Text model (ViT-L/16) and tokenizer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model, tokenizer = dinov3_vitl16_dinotxt_tet1280d20h24l()\n",
                "model.to(device)\n",
                "model.eval()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Sample Image\n",
                "We load a sample image from a URL."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_image_from_url(url: str) -> Image:\n",
                "    with urllib.request.urlopen(url) as f:\n",
                "        return Image.open(f).convert(\"RGB\")\n",
                "\n",
                "EXAMPLE_IMAGE_URL = \"https://dl.fbaipublicfiles.com/dinov2/images/example.jpg\"\n",
                "try:\n",
                "    img_pil = load_image_from_url(EXAMPLE_IMAGE_URL)\n",
                "except Exception as e:\n",
                "    print(\"Failed to load example image, falling back to dummy image.\")\n",
                "    img_pil = Image.new('RGB', (224, 224), color='red')\n",
                "\n",
                "# display(img_pil) # Uncomment in Jupyter"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Zero-Shot Classification\n",
                "Compute similarity between image global features and text descriptions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "image_preprocess = make_classification_eval_transform()\n",
                "image_tensor = torch.stack([image_preprocess(img_pil)], dim=0).to(device)\n",
                "\n",
                "texts = [\"photo of dogs\", \"photo of a chair\", \"photo of a bowl\", \"photo of a tupperware\"]\n",
                "class_names = [\"dog\", \"chair\", \"bowl\", \"tupperware\"]\n",
                "\n",
                "tokenized_texts_tensor = tokenizer.tokenize(texts).to(device)\n",
                "\n",
                "with torch.autocast(device_type=device, dtype=torch.float):\n",
                "    with torch.no_grad():\n",
                "        image_features = model.encode_image(image_tensor)\n",
                "        text_features = model.encode_text(tokenized_texts_tensor)\n",
                "\n",
                "# Normalize\n",
                "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
                "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
                "\n",
                "similarity = (\n",
                "    text_features.cpu().float().numpy() @ image_features.cpu().float().numpy().T\n",
                ")\n",
                "print(\"Similarity scores:\", similarity.flatten())\n",
                "\n",
                "# Best match\n",
                "best_idx = similarity.argmax()\n",
                "print(f\"Best match: {texts[best_idx]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Patch Embeddings & Alignment\n",
                "Visualize spatial alignment between text queries and image patches."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with torch.autocast(device_type=device, dtype=torch.float):\n",
                "    with torch.no_grad():\n",
                "        image_class_tokens, image_patch_tokens, backbone_patch_tokens = model.encode_image_with_patch_tokens(image_tensor)\n",
                "        # Part of text features that is aligned to patch features (e.g. from index 1024 onwards)\n",
                "        full_text_feats = model.encode_text(tokenized_texts_tensor)\n",
                "        text_features_aligned_to_patch = full_text_feats[:, 1024:]\n",
                "\n",
                "B, P, D = image_patch_tokens.shape\n",
                "H = W = int(P**0.5)\n",
                "\n",
                "x = image_patch_tokens.movedim(2, 1).unflatten(2, (H, W)).float()  # [B, D, H, W]\n",
                "x = F.interpolate(x, size=(480, 640), mode=\"bicubic\", align_corners=False)\n",
                "x = F.normalize(x, p=2, dim=1)\n",
                "\n",
                "y = F.normalize(text_features_aligned_to_patch.float(), p=2, dim=1)\n",
                "\n",
                "per_patch_similarity_to_text = torch.einsum(\"bdhw,cd->bchw\", x, y)\n",
                "\n",
                "# Argmax per pixel to see which text matches best where\n",
                "pred_idx = per_patch_similarity_to_text.argmax(1).squeeze(0)\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "plt.imshow(pred_idx.cpu().numpy())\n",
                "plt.title(\"Per-pixel Text Alignment\")\n",
                "plt.colorbar()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. ImageNet1k Zero-Shot Evaluation\n",
                "Full evaluation loop on ImageNet validation set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ImageNet Classes\n",
                "imagenet_clip_class_names= [\"tench\", \"goldfish\", \"great white shark\", \"tiger shark\", \"hammerhead shark\", \"electric ray\", \"stingray\", \"rooster\", \"hen\", \"ostrich\",\n",
                "    # ... (truncated for brevity, typically full list would be here)\n",
                "    \"goldfinch\", \"house finch\", \"junco\", \"indigo bunting\"\n",
                "]\n",
                "\n",
                "# Templates\n",
                "openai_imagenet_templates = (\n",
                "    lambda c: f\"a bad photo of a {c}.\",\n",
                "    lambda c: f\"a photo of many {c}.\",\n",
                "    lambda c: f\"a sculpture of a {c}.\",\n",
                "    lambda c: f\"a photo of the hard to see {c}.\",\n",
                "    lambda c: f\"a low resolution photo of the {c}.\",\n",
                "    lambda c: f\"a rendering of a {c}.\",\n",
                "    lambda c: f\"graffiti of a {c}.\",\n",
                "    lambda c: f\"a bad photo of the {c}.\",\n",
                "    lambda c: f\"a cropped photo of the {c}.\",\n",
                "    lambda c: f\"a tattoo of a {c}.\",\n",
                "    lambda c: f\"the embroidered {c}.\",\n",
                "    lambda c: f\"a photo of a hard to see {c}.\",\n",
                "    lambda c: f\"a bright photo of a {c}.\",\n",
                "    lambda c: f\"a photo of a clean {c}.\",\n",
                "    lambda c: f\"a photo of a dirty {c}.\",\n",
                "    lambda c: f\"a dark photo of the {c}.\",\n",
                "    lambda c: f\"a drawing of a {c}.\",\n",
                "    lambda c: f\"a photo of my {c}.\",\n",
                "    lambda c: f\"the plastic {c}.\",\n",
                "    # ... and more\n",
                ")\n",
                "\n",
                "from torchvision.datasets import ImageFolder\n",
                "\n",
                "def zeroshot_classifier(classnames, templates, tokenizer):\n",
                "    with torch.no_grad():\n",
                "        zeroshot_weights = []\n",
                "        for classname in classnames:\n",
                "            texts = [template(classname) for template in templates] #format with class\n",
                "            texts = tokenizer.tokenize(texts).to(device) #tokenize\n",
                "            class_embeddings = model.encode_text(texts) #embed with text encoder\n",
                "            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
                "            class_embedding = class_embeddings.mean(dim=0)\n",
                "            class_embedding /= class_embedding.norm()\n",
                "            zeroshot_weights.append(class_embedding)\n",
                "        zeroshot_weights = torch.stack(zeroshot_weights, dim=1).to(device)\n",
                "    return zeroshot_weights\n",
                "\n",
                "def accuracy(output, target, topk=(1,)):\n",
                "    pred = output.topk(max(topk), 1, True, True)[1].t()\n",
                "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
                "    return [correct[:k].reshape(-1).sum(0, keepdim=True) for k in topk]\n",
                "\n",
                "# Please update the following directory to the root of ImageNet1k val dataset.\n",
                "imagenet_val_root_dir = \"./imagenet_val_dummy\" # Placeholder path\n",
                "\n",
                "if os.path.exists(imagenet_val_root_dir):\n",
                "    print(\"Starting ImageNet Evaluation...\")\n",
                "\n",
                "    zeroshot_weights = zeroshot_classifier(imagenet_clip_class_names, openai_imagenet_templates, tokenizer)\n",
                "\n",
                "    val_dataset = ImageFolder(imagenet_val_root_dir, image_preprocess)\n",
                "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
                "\n",
                "    top1, top5, n = 0., 0., 0.\n",
                "    for images, targets in val_loader:\n",
                "        with torch.autocast(device_type=device, dtype=torch.float):\n",
                "            with torch.no_grad():\n",
                "                image_features = model.encode_image(images.to(device))\n",
                "                image_features /= image_features.norm(dim=-1, keepdim=True)\n",
                "                logits = 100. * image_features @ zeroshot_weights\n",
                "                acc1, acc5 = accuracy(logits, targets.to(device), topk=(1, 5))\n",
                "                top1 += acc1\n",
                "                top5 += acc5\n",
                "                n += len(images)\n",
                "    \n",
                "    top1 = (top1.item() / n) * 100\n",
                "    top5 = (top5.item() / n) * 100 \n",
                "    print(f\"Top-1 accuracy: {top1}\")\n",
                "    print(f\"Top-5 accuracy: {top5}\")\n",
                "else:\n",
                "    print(\"ImageNet validation directory not found. Skipping full evaluation.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}