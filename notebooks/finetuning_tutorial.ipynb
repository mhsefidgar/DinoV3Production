{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DINOv3 Finetuning Tutorial (LoRA)\n",
                "\n",
                "This tutorial demonstrates how to efficiently finetune DINOv3 using **Low-Rank Adaptation (LoRA)**.\n",
                "\n",
                "Ideally suited for adapting the powerful DINOv3 features to downstream tasks like classification or segmentation with minimal parameter updates.\n",
                "\n",
                "**Agenda:**\n",
                "1. **Setup**: Load model and apply LoRA.\n",
                "2. **Training Loop**: Simulate a training process with an optimizer and loss tracking.\n",
                "3. **Analysis**: Plot the loss curve.\n",
                "4. **Inference**: Run inference with the adapted model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "from dinov3production import create_model\n",
                "from dinov3production.finetune.peft import wrap_with_lora\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Model Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Base Model\n",
                "model = create_model('dinov3_vitb14', pretrained=True).to(device)\n",
                "\n",
                "# Apply LoRA with rank=16\n",
                "peft_model = wrap_with_lora(model, r=16)\n",
                "peft_model.to(device)\n",
                "\n",
                "# Verify Trainable Parameters\n",
                "peft_model.print_trainable_parameters()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Simulated Training Loop\n",
                "We will fine-tune the model on a dummy classification task (10 classes)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add a classifier head for demonstration (Linear Probe style)\n",
                "# Note: In a real scenario, you'd attach a head. Here we assume the model output is features.\n",
                "# Let's wrap it in a simple Module to include a head.\n",
                "class Classifier(nn.Module):\n",
                "    def __init__(self, backbone, num_classes=10):\n",
                "        super().__init__()\n",
                "        self.backbone = backbone\n",
                "        self.head = nn.Linear(backbone.embed_dim, num_classes)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        features = self.backbone(x)\n",
                "        # Assuming backbone returns [B, D] or [B, N, D]. If [B, N, D], take CLS.\n",
                "        if features.dim() == 3:\n",
                "            features = features[:, 0]\n",
                "        return self.head(features)\n",
                "\n",
                "finetune_model = Classifier(peft_model).to(device)\n",
                "\n",
                "# Train only the LoRA params and the head\n",
                "optimizer = optim.AdamW([\n",
                "    {'params': peft_model.parameters(), 'lr': 1e-4},\n",
                "    {'params': finetune_model.head.parameters(), 'lr': 1e-3}\n",
                "])\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "\n",
                "# Dummy Data Loop\n",
                "losses = []\n",
                "print(\"Starting Training...\")\n",
                "for step in range(50):\n",
                "    # Dummy Batch\n",
                "    inputs = torch.randn(4, 3, 224, 224).to(device)\n",
                "    targets = torch.randint(0, 10, (4,)).to(device)\n",
                "    \n",
                "    optimizer.zero_grad()\n",
                "    outputs = finetune_model(inputs)\n",
                "    loss = criterion(outputs, targets)\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "    \n",
                "    losses.append(loss.item())\n",
                "    if step % 10 == 0:\n",
                "        print(f\"Step {step}: Loss {loss.item():.4f}\")\n",
                "\n",
                "print(\"Training Complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Analysis\n",
                "Visualize the training loss."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 5))\n",
                "plt.plot(losses, label=\"Training Loss\")\n",
                "plt.xlabel(\"Step\")\n",
                "plt.ylabel(\"Loss\")\n",
                "plt.title(\"LoRA Finetuning Loss\")\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Inference\n",
                "Run the finetuned model on a new input."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "finetune_model.eval()\n",
                "with torch.no_grad():\n",
                "    test_input = torch.randn(1, 3, 224, 224).to(device)\n",
                "    logits = finetune_model(test_input)\n",
                "    probs = torch.softmax(logits, dim=-1)\n",
                "    pred_class = torch.argmax(probs, dim=-1).item()\n",
                "    \n",
                "print(f\"Predicted Class: {pred_class}\")\n",
                "print(f\"Probabilities: {probs[0].cpu().numpy().round(2)}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}