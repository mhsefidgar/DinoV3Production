{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Training a Foreground Segmentation Tool with DINOv3\n",
                "\n",
                "In this tutorial, we will train a linear foreground segmentation model using DINOv3 features. We use real sample images."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import io\n",
                "import os\n",
                "import pickle\n",
                "import tarfile\n",
                "import urllib\n",
                "\n",
                "from PIL import Image\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from scipy import signal\n",
                "\n",
                "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "\n",
                "import torch\n",
                "import torchvision.transforms.functional as TF\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Library Imports\n",
                "from dinov3production import create_model\n",
                "from dinov3production.data.transforms import resize_to_patch_multiple, quantize_mask\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "\n",
                "# Load Model\n",
                "# We use ViT-S/14 for Colab stability (low memory usage).\n",
                "# Switch to 'dinov3_vitl14' (Large) if you have enough GPU RAM.\n",
                "model = create_model('dinov3_vits14', pretrained=True)\n",
                "model.to(device)\n",
                "model.eval()\n",
                "\n",
                "PATCH_SIZE = 14 # Aligned with model architecture\n",
                "IMAGE_SIZE = 768"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading\n",
                "We load real images from the web. Since we need ground truth masks for training, we will use a small set of images where we can approximate the foreground or download corresponding masks if available. For this tutorial, we visualize the images."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_image_from_url(url: str) -> Image:\n",
                "    with urllib.request.urlopen(url) as f:\n",
                "        return Image.open(f).convert(\"RGB\")\n",
                "\n",
                "# Define a small training set of Real Images\n",
                "DATA_URLS = [\n",
                "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/American_Eskimo_Dog.jpg/600px-American_Eskimo_Dog.jpg\",\n",
                "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Aurora_as_seen_from_Fairbanks_Alaska.jpg/600px-Aurora_as_seen_from_Fairbanks_Alaska.jpg\",\n",
                "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Mandel_zoom_00_mandelbrot_set.jpg/600px-Mandel_zoom_00_mandelbrot_set.jpg\",\n",
                "]\n",
                "\n",
                "images = []\n",
                "labels = []\n",
                "\n",
                "print(\"Downloading real samples...\")\n",
                "for i, url in enumerate(DATA_URLS):\n",
                "    try:\n",
                "        img = load_image_from_url(url)\n",
                "        images.append(img)\n",
                "        \n",
                "        # Generate a 'Weak Label' mask for tutorial purposes\n",
                "        # In a real app, you would have hand-labeled masks (black/white PNGs)\n",
                "        # Here we assume central object bias for the Dog/Mandelbrot, or just a dummy center box\n",
                "        w, h = img.size\n",
                "        mask = Image.new('L', (w, h), 0)\n",
                "        # Draw a box in the center as 'foreground' label\n",
                "        margin_w, margin_h = int(w*0.25), int(h*0.25)\n",
                "        for y in range(margin_h, h - margin_h):\n",
                "            for x in range(margin_w, w - margin_w):\n",
                "                mask.putpixel((x, y), 255)\n",
                "                \n",
                "        labels.append(mask)\n",
                "    except Exception as e:\n",
                "        print(f\"Failed to download {url}: {e}\")\n",
                "\n",
                "n_images = len(images)\n",
                "print(f\"Loaded {n_images} images.\")\n",
                "\n",
                "# Visualize one example\n",
                "if n_images > 0:\n",
                "    plt.subplot(1, 2, 1); plt.imshow(images[0]); plt.title(\"Image\")\n",
                "    plt.subplot(1, 2, 2); plt.imshow(labels[0]); plt.title(\"Weak Label Mask\")\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Feature Extraction & Label Building\n",
                "Resize images/masks to patch grid, quantized mask, extract features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "xs = []\n",
                "ys = []\n",
                "image_index = []\n",
                "\n",
                "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
                "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
                "\n",
                "def extract_dino_features(model, x):\n",
                "    # x: [B, 3, H, W]\n",
                "    B, _, H, W = x.shape\n",
                "    with torch.cuda.amp.autocast(enabled=True):\n",
                "        out = model.forward_features(x)\n",
                "    \n",
                "    # Get number of register tokens\n",
                "    n_reg = getattr(model, 'num_register_tokens', 0)\n",
                "    \n",
                "    # Slice: [CLS, REG_1...REG_K, PATCHES...]\n",
                "    patch_tokens = out[:, 1+n_reg:]\n",
                "    \n",
                "    h = H // PATCH_SIZE\n",
                "    w = W // PATCH_SIZE\n",
                "    feats = patch_tokens.reshape(B, h, w, -1)\n",
                "    return feats.permute(0, 3, 1, 2) # [B, D, H, W]\n",
                "\n",
                "with torch.inference_mode():\n",
                "     for i in tqdm(range(n_images), desc=\"Processing images\"):\n",
                "         # Process Label: Resize & Quantize\n",
                "         mask_i = labels[i].split()[-1] # Extract alpha/BW channel\n",
                "         mask_i_resized = resize_to_patch_multiple(mask_i, PATCH_SIZE, IMAGE_SIZE)\n",
                "         mask_i_quantized = quantize_mask(mask_i_resized, PATCH_SIZE)\n",
                "         ys.append(mask_i_quantized.view(-1).cpu())\n",
                "         \n",
                "         # Process Image: Resize & Norm\n",
                "         image_i = images[i].convert('RGB')\n",
                "         image_i_resized = resize_to_patch_multiple(image_i, PATCH_SIZE, IMAGE_SIZE)\n",
                "         image_i_norm = TF.normalize(image_i_resized, mean=IMAGENET_MEAN, std=IMAGENET_STD).unsqueeze(0).to(device)\n",
                "         \n",
                "         # Real Feature Extraction\n",
                "         feats = extract_dino_features(model, image_i_norm)\n",
                "         \n",
                "         dim = feats.shape[1]\n",
                "         xs.append(feats.squeeze().view(dim, -1).permute(1,0).cpu())\n",
                "         \n",
                "         image_index.append(i * torch.ones(ys[-1].shape))\n",
                "\n",
                "if len(xs) > 0:\n",
                "    xs = torch.cat(xs)\n",
                "    ys = torch.cat(ys)\n",
                "    image_index = torch.cat(image_index)\n",
                "\n",
                "    # Filter ambiguous labels (edges)\n",
                "    idx = (ys < 0.01) | (ys > 0.99)\n",
                "    xs = xs[idx]\n",
                "    ys = ys[idx]\n",
                "    image_index = image_index[idx]\n",
                "    \n",
                "    print(\"Design matrix:\", xs.shape)\n",
                "    print(\"Label matrix:\", ys.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Leave-One-Out Cross-Validation\n",
                "Train LRs with different C values on N-1 images, test on 1."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if len(xs) > 0:\n",
                "    cs = np.logspace(-7, 0, 8)\n",
                "    scores = np.zeros((n_images, len(cs)))\n",
                "\n",
                "    for i in range(n_images):\n",
                "        print(f'Validation using image_{i+1:02d}')\n",
                "        \n",
                "        # Train/Val Split\n",
                "        train_selection = image_index != float(i)\n",
                "        fold_x = xs[train_selection].numpy()\n",
                "        fold_y = (ys[train_selection] > 0).long().numpy()\n",
                "        val_x = xs[~train_selection].numpy()\n",
                "        val_y = (ys[~train_selection] > 0).long().numpy()\n",
                "\n",
                "        for j, c in enumerate(cs):\n",
                "             # print(f\"Training C={c:.2e}\")\n",
                "             clf = LogisticRegression(random_state=0, C=c, max_iter=1000).fit(fold_x, fold_y)\n",
                "             output = clf.predict_proba(val_x)\n",
                "             s = average_precision_score(val_y, output[:, 1])\n",
                "             scores[i, j] = s\n",
                "\n",
                "    # Plot Average Scores to find best C\n",
                "    plt.figure(figsize=(5, 3))\n",
                "    plt.plot(scores.mean(axis=0))\n",
                "    plt.xticks(np.arange(len(cs)), [f\"{c:.0e}\" for c in cs])\n",
                "    plt.xlabel('C')\n",
                "    plt.ylabel('Average AP')\n",
                "    plt.grid()\n",
                "    plt.title(\"Cross-Validation Results\")\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Final Training & Saving\n",
                "Train with optimal C (usually 0.1 or 1.0) on all data and save."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if len(xs) > 0:\n",
                "    # Picking C=0.1 as per tutorial suggestion\n",
                "    print(\"Retraining with C=0.1 on full dataset...\")\n",
                "    final_clf = LogisticRegression(random_state=0, C=0.1, max_iter=5000).fit(xs.numpy(), (ys > 0).long().numpy())\n",
                "    \n",
                "    # Save\n",
                "    with open(\"fg_classifier.pkl\", \"wb\") as f:\n",
                "        pickle.dump(final_clf, f)\n",
                "    print(\"Saved fg_classifier.pkl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Test Inference with Median Filter"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "TEST_IMAGE_URI = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d7/Green_Sea_Turtle_grazing_seagrass.jpg/640px-Green_Sea_Turtle_grazing_seagrass.jpg\"\n",
                "\n",
                "def load_image_from_url(url: str) -> Image:\n",
                "    try:\n",
                "        with urllib.request.urlopen(url) as f:\n",
                "            return Image.open(f).convert(\"RGB\")\n",
                "    except:\n",
                "        return Image.new('RGB', (500, 500), color='blue')\n",
                "\n",
                "test_img = load_image_from_url(TEST_IMAGE_URI)\n",
                "test_img_resized = resize_to_patch_multiple(test_img, PATCH_SIZE, IMAGE_SIZE)\n",
                "test_norm = TF.normalize(test_img_resized, mean=IMAGENET_MEAN, std=IMAGENET_STD).unsqueeze(0).to(device)\n",
                "\n",
                "with torch.inference_mode():\n",
                "    # Real feature extraction for test image\n",
                "    feats = extract_dino_features(model, test_norm)\n",
                "\n",
                "    x_test = feats.squeeze().view(dim, -1).permute(1, 0).cpu().numpy()\n",
                "    \n",
                "    h, w = test_img_resized.shape[1] // PATCH_SIZE, test_img_resized.shape[2] // PATCH_SIZE\n",
                "\n",
                "if 'final_clf' in locals():\n",
                "    probs = final_clf.predict_proba(x_test)[:, 1].reshape(h, w)\n",
                "    probs_mf = signal.medfilt2d(probs, kernel_size=3)\n",
                "\n",
                "    plt.figure(figsize=(10, 4))\n",
                "    plt.subplot(1, 3, 1); plt.imshow(test_img); plt.title(\"Input\")\n",
                "    plt.subplot(1, 3, 2); plt.imshow(probs); plt.title(\"Raw Probs\")\n",
                "    plt.subplot(1, 3, 3); plt.imshow(probs_mf); plt.title(\"+ Median Filter\")\n",
                "    plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}