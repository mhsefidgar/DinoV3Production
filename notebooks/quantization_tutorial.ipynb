{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DINOv3 Quantization Tutorial\n",
                "\n",
                "This tutorial demonstrates how to optimize DINOv3 models using Int4 quantization with `torchao`.\n",
                "\n",
                "We will covers:\n",
                "1. **Model Loading**: Loading a standard DINOv3 ViT-B model.\n",
                "2. **Quantization**: Compressing the model weights to 4-bit integers.\n",
                "3. **Benchmarking**: Comparing Memory usage and Latency between FP32 and Int4.\n",
                "4. **Visualization**: verifying that the attention maps (and thus semantic understanding) are preserved."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import time\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from dinov3production import create_model\n",
                "from dinov3production.quantize.quantizer import Quantizer\n",
                "import dinov3production.visualization.image as viz\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Pretrained Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_name = 'dinov3_vitb14'\n",
                "model = create_model(model_name, pretrained=True).to(device).eval()\n",
                "print(f\"Loaded {model_name}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Quantization (Int4)\n",
                "Use the `Quantizer` utility to apply Int4 weight-only quantization."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "quantizer = Quantizer(model)\n",
                "q_model = quantizer.to_int4()\n",
                "print(\"Model quantized to Int4.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Benchmarking\n",
                "Let's measure the improvements. We expect lower memory usage and potentially faster inference on newer efficient hardware."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def benchmark(model, input_tensor, n_warmup=10, n_runs=100):\n",
                "    # Warmup\n",
                "    for _ in range(n_warmup):\n",
                "        _ = model(input_tensor)\n",
                "    \n",
                "    torch.cuda.synchronize()\n",
                "    start_time = time.perf_counter()\n",
                "    \n",
                "    for _ in range(n_runs):\n",
                "        _ = model(input_tensor)\n",
                "        \n",
                "    torch.cuda.synchronize()\n",
                "    end_time = time.perf_counter()\n",
                "    \n",
                "    avg_time_ms = (end_time - start_time) * 1000 / n_runs\n",
                "    return avg_time_ms\n",
                "\n",
                "def get_model_size_mb(model):\n",
                "    param_size = 0\n",
                "    for param in model.parameters():\n",
                "        param_size += param.nelement() * param.element_size()\n",
                "    \n",
                "    buffer_size = 0\n",
                "    for buffer in model.buffers():\n",
                "        buffer_size += buffer.nelement() * buffer.element_size()\n",
                "\n",
                "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
                "    return size_all_mb\n",
                "\n",
                "# Create Dummy Input\n",
                "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
                "\n",
                "# Compare sizes (Approximate for Int4 as torch sees float wrapper often, but let's check)\n",
                "print(f\"Original Model Size: {get_model_size_mb(model):.2f} MB\")\n",
                "print(f\"Quantized Model Size: {get_model_size_mb(q_model):.2f} MB (Estimated compressed)\")\n",
                "\n",
                "# Measure Latency\n",
                "lat_fp32 = benchmark(model, dummy_input)\n",
                "lat_int4 = benchmark(q_model, dummy_input)\n",
                "\n",
                "print(f\"FP32 Latency: {lat_fp32:.2f} ms\")\n",
                "print(f\"Int4 Latency: {lat_int4:.2f} ms\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Visualization\n",
                "To ensure quality is preserved, let's visualize the attention maps of the last block for both models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load a real image or use dummy noise\n",
                "try:\n",
                "    import requests\n",
                "    from io import BytesIO\n",
                "    from PIL import Image\n",
                "    url = \"https://dl.fbaipublicfiles.com/dinov2/images/example.jpg\"\n",
                "    response = requests.get(url)\n",
                "    img_pil = Image.open(BytesIO(response.content))\n",
                "except:\n",
                "    img_pil = Image.new('RGB', (224, 224), color='red')\n",
                "\n",
                "# Preprocess\n",
                "import torchvision.transforms as T\n",
                "transform = T.Compose([\n",
                "    T.Resize((224, 224)),\n",
                "    T.ToTensor(),\n",
                "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
                "])\n",
                "img_tensor = transform(img_pil).unsqueeze(0).to(device)\n",
                "\n",
                "# Hook to capture attention\n",
                "def get_attention(m, x):\n",
                "    # This is a simplification. \n",
                "    # For DINOv3, we can use get_intermediate_layers or get_last_selfattention\n",
                "    # Assuming the method exists on the ViT model\n",
                "    return m.get_last_selfattention(x)\n",
                "\n",
                "# Note: The provided API wrapper might need direct access. \n",
                "# If get_last_selfattention exists:\n",
                "try:\n",
                "    with torch.inference_mode():\n",
                "        att_fp32 = model.get_last_selfattention(img_tensor)\n",
                "        att_int4 = q_model.get_last_selfattention(img_tensor)\n",
                "        \n",
                "    # Keep only CLS attention for visualization [B, H, N, N] -> [B, H, 0, 1:] reshaped\n",
                "    # Visualize mean head attention for patch tokens wrt CLS token\n",
                "    nh = att_fp32.shape[1]\n",
                "    \n",
                "    # [0, :, 0, 1:] -> Mean over heads -> Reshape to 16x16\n",
                "    att_map_fp32 = att_fp32[0, :, 0, 1:].mean(0).reshape(14, 14).cpu().numpy()\n",
                "    att_map_int4 = att_int4[0, :, 0, 1:].mean(0).reshape(14, 14).cpu().numpy()\n",
                "    \n",
                "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
                "    ax[0].imshow(img_pil.resize((224, 224)))\n",
                "    ax[0].set_title(\"Input Image\")\n",
                "    \n",
                "    ax[1].imshow(viz.visualize_attention(np.array(img_pil.resize((224, 224))), att_map_fp32))\n",
                "    ax[1].set_title(\"FP32 Attention\")\n",
                "    \n",
                "    ax[2].imshow(viz.visualize_attention(np.array(img_pil.resize((224, 224))), att_map_int4))\n",
                "    ax[2].set_title(\"Int4 Attention\")\n",
                "    plt.show()\n",
                "    \n",
                "except AttributeError:\n",
                "    print(\"Model does not support get_last_selfattention directly or is wrapped unpredictably.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}