{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Segmentation Tracking with DINOv3\n",
                "\n",
                "This notebook demonstrates using DINOv3 for video segmentation tracking using a non-parametric method similar to \"Space-time correspondence as a contrastive random walk\" (Jabri et al. 2020).\n",
                "\n",
                "Given:\n",
                "- RGB video frames\n",
                "- Instance segmentation masks for the first frame\n",
                "\n",
                "We will extract patch features from each frame and use patch similarity to propagate the ground-truth labels to all frames."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup\n",
                "Let's start by loading some pre-requisites, setting up the environment and checking the DINOv3 repository location:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import datetime\n",
                "import functools\n",
                "import io\n",
                "import logging\n",
                "import math\n",
                "import os\n",
                "from pathlib import Path\n",
                "import tarfile\n",
                "import time\n",
                "import urllib\n",
                "\n",
                "import lovely_tensors\n",
                "import matplotlib.pyplot as plt\n",
                "import mediapy as mp\n",
                "import numpy as np\n",
                "from PIL import Image\n",
                "import torch\n",
                "import torch.nn.functional as F\n",
                "import torchvision.transforms as TVT\n",
                "import torchvision.transforms.functional as TVTF\n",
                "from torch import Tensor, nn\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Library Imports\n",
                "from dinov3production import create_model\n",
                "from dinov3production.video.tracking import propagate, make_neighborhood_mask\n",
                "\n",
                "DISPLAY_HEIGHT = 200\n",
                "lovely_tensors.monkey_patch()\n",
                "torch.set_grad_enabled(False)\n",
                "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Model\n",
                "We load the DINOv3 ViT-S model (Small) for Colab stability."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Model\n",
                "# We use ViT-S/14 for Colab stability (low memory usage).\n",
                "# You can switch to 'dinov3_vitl14' (Large) or 'dinov3_vitg14' (Giant) if you have enough VRAM.\n",
                "model = create_model('dinov3_vits14', pretrained=True)\n",
                "model.to(\"cuda\")\n",
                "model.eval()\n",
                "\n",
                "patch_size = 14\n",
                "embed_dim = 384 # 1024 for Large\n",
                "\n",
                "print(f\"Patch size: {patch_size}\")\n",
                "print(f\"Embedding dimension: {embed_dim}\")\n",
                "print(f\"Peak GPU memory: {torch.cuda.max_memory_allocated() / 2**30:.1f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We want to process one image at the time and get L2-normalized features. Here is a wrapper to do just that."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@torch.compile(disable=True)\n",
                "def extract_dino_features(model, img):\n",
                "    # img: [3, H, W] -> unsqueeze -> [1, 3, H, W]\n",
                "    x = img.unsqueeze(0)\n",
                "    B, _, H, W = x.shape\n",
                "    \n",
                "    # Forward pass\n",
                "    with torch.cuda.amp.autocast(enabled=True):\n",
                "        out = model.forward_features(x)\n",
                "        \n",
                "    # Handle Register Tokens\n",
                "    n_reg = getattr(model, 'num_register_tokens', 0)\n",
                "    \n",
                "    # Slice: [CLS, REG_1...REG_K, PATCHES...]\n",
                "    patch_tokens = out[:, 1+n_reg:]\n",
                "    \n",
                "    # Reshape\n",
                "    h = H // patch_size\n",
                "    w = W // patch_size\n",
                "    feats = patch_tokens.reshape(B, h, w, -1)\n",
                "    \n",
                "    # Normalize\n",
                "    feats = F.normalize(feats, dim=-1, p=2)\n",
                "    return feats.squeeze(0) # [h, w, D]\n",
                "\n",
                "def forward(model, img):\n",
                "    return extract_dino_features(model, img)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data\n",
                "Here we load the video frames. To ensure this tutorial works for everyone, we download a single high-resolution panoramic image and simulate a video by panning across it. This serves as a robust 'Real Sample' of video motion."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "PANO_URI = \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Panoramic_view_of_London_from_One_Canada_Square.jpg/1280px-Panoramic_view_of_London_from_One_Canada_Square.jpg\"\n",
                "\n",
                "def load_image_from_url(url: str) -> Image:\n",
                "    try:\n",
                "        with urllib.request.urlopen(url) as f:\n",
                "            return Image.open(f).convert(\"RGB\")\n",
                "    except:\n",
                "        print(\"Download failed, creating dummy.\")\n",
                "        return Image.new('RGB', (1280, 720), color='gray')\n",
                "\n",
                "pano = load_image_from_url(PANO_URI)\n",
                "print(f\"Downloaded panorama: {pano.size}\")\n",
                "\n",
                "# Create video sequence (Panning)\n",
                "frames = []\n",
                "num_frames = 10\n",
                "width, height = pano.size\n",
                "crop_w, crop_h = 480, 320\n",
                "\n",
                "for i in range(num_frames):\n",
                "    # Linear pan from left to midway\n",
                "    offset_x = int(i * (width - crop_w) / (num_frames * 2)) \n",
                "    box = (offset_x, 0, offset_x + crop_w, crop_h)\n",
                "    frame = pano.crop(box)\n",
                "    frames.append(frame)\n",
                "\n",
                "print(f\"Generated {len(frames)} frames from panorama.\")\n",
                "\n",
                "if num_frames > 0:\n",
                "    original_width, original_height = frames[0].size\n",
                "    print(f\"Frame size: width={original_width}, height={original_height}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's show four sample frames from the video:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if num_frames > 0:\n",
                "    num_selected_frames = 4\n",
                "    selected_frames = np.linspace(0, num_frames - 1, num_selected_frames, dtype=int)\n",
                "\n",
                "    mp.show_images(\n",
                "        [frames[int(i)] for i in selected_frames],\n",
                "        titles=[f\"Frame {i}\" for i in selected_frames],\n",
                "        height=DISPLAY_HEIGHT,\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This notebook assumes that instance segmentation masks for the first frame are stored in a .png file. We will generate one for our panning video: a circle in the center of the first frame."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def mask_to_rgb(mask: np.ndarray | Tensor, num_masks: int) -> np.ndarray:\n",
                "    if isinstance(mask, Tensor):\n",
                "        mask = mask.cpu().numpy()\n",
                "\n",
                "    # Exclude background\n",
                "    background = mask == 0\n",
                "    mask = mask - 1\n",
                "    num_masks = num_masks - 1\n",
                "\n",
                "    # Choose palette\n",
                "    if num_masks <= 10:\n",
                "        mask_rgb = plt.get_cmap(\"tab10\")(mask)[..., :3]\n",
                "    elif num_masks <= 20:\n",
                "        mask_rgb = plt.get_cmap(\"tab20\")(mask)[..., :3]\n",
                "    else:\n",
                "        mask_rgb = plt.get_cmap(\"gist_rainbow\")(mask / (num_masks - 1))[..., :3]\n",
                "\n",
                "    mask_rgb = (mask_rgb * 255).astype(np.uint8)\n",
                "    mask_rgb[background, :] = 0\n",
                "    return mask_rgb\n",
                "\n",
                "# Create a dummy mask for the first frame (circle)\n",
                "first_mask_np = np.zeros((crop_h, crop_w), dtype=np.uint8)\n",
                "cy, cx = crop_h // 2, crop_w // 2\n",
                "r = 60\n",
                "y, x = np.ogrid[:crop_h, :crop_w]\n",
                "mask_area = (x - cx)**2 + (y - cy)**2 <= r**2\n",
                "first_mask_np[mask_area] = 1 # Label 1\n",
                "\n",
                "mask_height, mask_width = first_mask_np.shape \n",
                "print(f\"Mask size: {[mask_height, mask_width]}\")\n",
                "\n",
                "num_masks = 2 # BG (0) + Circle (1)\n",
                "print(f\"Number of masks: {num_masks}\")\n",
                "\n",
                "if num_frames > 0:\n",
                "    mp.show_images(\n",
                "        [frames[0], mask_to_rgb(first_mask_np, num_masks)],\n",
                "        titles=[\"Frame\", \"Mask\"],\n",
                "        height=DISPLAY_HEIGHT,\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Transforms\n",
                "Input frames need to be resized to match the desired forward resolution and the model patch size."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ResizeToMultiple(nn.Module):\n",
                "    def __init__(self, short_side: int, multiple: int):\n",
                "        super().__init__()\n",
                "        self.short_side = short_side\n",
                "        self.multiple = multiple\n",
                "\n",
                "    def _round_up(self, side: float) -> int:\n",
                "        return math.ceil(side / self.multiple) * self.multiple\n",
                "\n",
                "    def forward(self, img):\n",
                "        old_width, old_height = TVTF.get_image_size(img)\n",
                "        if old_width > old_height:\n",
                "            new_height = self._round_up(self.short_side)\n",
                "            new_width = self._round_up(old_width * new_height / old_height)\n",
                "        else:\n",
                "            new_width = self._round_up(self.short_side)\n",
                "            new_height = self._round_up(old_height * new_width / old_width)\n",
                "        return TVTF.resize(img, [new_height, new_width], interpolation=TVT.InterpolationMode.BICUBIC)\n",
                "\n",
                "\n",
                "SHORT_SIDE = 480 # Reduced for Speed/Safety\n",
                "\n",
                "transform = TVT.Compose(\n",
                "    [\n",
                "        ResizeToMultiple(short_side=SHORT_SIDE, multiple=patch_size),\n",
                "        TVT.ToTensor(),\n",
                "        TVT.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "    ]\n",
                ")\n",
                "if num_frames > 0:\n",
                "    first_frame = transform(frames[0]).to(\"cuda\")\n",
                "    print(f\"First frame: {first_frame}\")\n",
                "\n",
                "    _, frame_height, frame_width = first_frame.shape\n",
                "    feats_height, feats_width = frame_height // patch_size, frame_width // patch_size"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Label propagation happens at the output resolution of the model, so we downsample the ground-truth masks of the first frame and turn them into a one-hot probability map."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "first_mask = torch.from_numpy(first_mask_np).to(\"cuda\", dtype=torch.long)\n",
                "first_mask = F.interpolate(\n",
                "    first_mask[None, None, :, :].float(),\n",
                "    (feats_height, feats_width),\n",
                "    mode=\"nearest-exact\",\n",
                ")[0, 0].long()\n",
                "\n",
                "first_probs = F.one_hot(first_mask, num_masks).float()\n",
                "print(f\"First mask shape: {first_mask.shape}\")\n",
                "print(f\"First probs shape: {first_probs.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## How it works\n",
                "Label propagation takes as input current features, context features, and probabilities, and computes similarity."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Using library implementations for optimized propagation\n",
                "from dinov3production.video.tracking import propagate, make_neighborhood_mask"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Visualization of neighborhood mask:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "neighborhood_mask = make_neighborhood_mask(feats_height, feats_width, size=12, shape=\"circle\")\n",
                "\n",
                "mp.show_images(\n",
                "    {f\"{(i, j)}\": neighborhood_mask[i, j].cpu().numpy() for i, j in [[3, 14], [20, 25]]},\n",
                "    height=DISPLAY_HEIGHT,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To understand how it works, let's do it for one frame only. The \"context\" contains only the first frame and the \"current frame\" is the second one."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if num_frames > 1:\n",
                "    torch._dynamo.maybe_mark_dynamic(first_frame, (1, 2))\n",
                "    first_feats = forward(model, first_frame)\n",
                "\n",
                "    frame_idx = 1\n",
                "    current_frame_pil = frames[frame_idx]\n",
                "    current_frame = transform(current_frame_pil).to(\"cuda\")\n",
                "    torch._dynamo.maybe_mark_dynamic(current_frame, (1, 2))\n",
                "    current_feats = forward(model, current_frame)\n",
                "\n",
                "    current_probs = propagate(\n",
                "        current_feats,\n",
                "        context_features=first_feats.unsqueeze(0),\n",
                "        context_probs=first_probs.unsqueeze(0),\n",
                "        neighborhood_mask=neighborhood_mask,\n",
                "        topk=5,\n",
                "        temperature=0.2,\n",
                "    )\n",
                "    print(f\"Current probs shape: {current_probs.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Then, we upsample the predicted probabilities and postprocess them."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def postprocess_probs(probs: Tensor) -> Tensor:\n",
                "    vmin = probs.flatten(2, 3).min(dim=2).values\n",
                "    vmax = probs.flatten(2, 3).max(dim=2).values\n",
                "    probs = (probs - vmin[:, :, None, None]) / (vmax[:, :, None, None] - vmin[:, :, None, None])\n",
                "    probs = torch.nan_to_num(probs, nan=0)\n",
                "    return probs\n",
                "\n",
                "if num_frames > 1:\n",
                "    p = current_probs.movedim(-1, -3).unsqueeze(0)\n",
                "    p = F.interpolate(p, size=(mask_height, mask_width), mode=\"nearest\")\n",
                "    p = postprocess_probs(p).squeeze(0)\n",
                "    current_pred_np = p.argmax(0).cpu().numpy()\n",
                "    current_probs_np = p.cpu().numpy()\n",
                "\n",
                "    mp.show_images(\n",
                "        [\n",
                "            frames[0],\n",
                "            current_frame_pil,\n",
                "            mask_to_rgb(first_mask_np, num_masks),\n",
                "            mask_to_rgb(current_pred_np, num_masks),\n",
                "        ],\n",
                "        titles=[\"First frame\", \"Second frame\", \"\", \"\"],\n",
                "        columns=2,\n",
                "        height=DISPLAY_HEIGHT,\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Process Video\n",
                "Process all frames with context queue."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "MAX_CONTEXT_LENGTH = 7\n",
                "NEIGHBORHOOD_SIZE = 12\n",
                "NEIGHBORHOOD_SHAPE = \"circle\"\n",
                "TOPK = 5\n",
                "TEMPERATURE = 0.2\n",
                "\n",
                "mask_predictions = torch.zeros([num_frames, mask_height, mask_width], dtype=torch.uint8)\n",
                "mask_predictions[0, :, :] = torch.from_numpy(first_mask_np)\n",
                "\n",
                "mask_probabilities = torch.zeros([num_frames, num_masks, mask_height, mask_width])\n",
                "mask_probabilities[0, :, :, :] = F.one_hot(torch.from_numpy(first_mask_np).long(), num_masks).movedim(-1, -3)\n",
                "\n",
                "features_queue: list[Tensor] = []\n",
                "probs_queue: list[Tensor] = []\n",
                "\n",
                "neighborhood_mask = make_neighborhood_mask(\n",
                "    feats_height,\n",
                "    feats_width,\n",
                "    size=NEIGHBORHOOD_SIZE,\n",
                "    shape=NEIGHBORHOOD_SHAPE,\n",
                ")\n",
                "\n",
                "if num_frames > 1:\n",
                "    start = time.perf_counter()\n",
                "    for frame_idx in tqdm(range(1, num_frames), desc=\"Processing\"):\n",
                "        current_frame_pil = frames[frame_idx]\n",
                "        current_frame = transform(current_frame_pil).to(\"cuda\")\n",
                "        torch._dynamo.maybe_mark_dynamic(current_frame, (1, 2))\n",
                "        current_feats = forward(model, current_frame)\n",
                "\n",
                "        context_feats = torch.stack([first_feats, *features_queue], dim=0)\n",
                "        context_probs = torch.stack([first_probs, *probs_queue], dim=0)\n",
                "        torch._dynamo.maybe_mark_dynamic(context_feats, 0)\n",
                "        torch._dynamo.maybe_mark_dynamic(context_probs, (0, 3))\n",
                "\n",
                "        current_probs = propagate(\n",
                "            current_feats,\n",
                "            context_feats,\n",
                "            context_probs,\n",
                "            neighborhood_mask,\n",
                "            TOPK,\n",
                "            TEMPERATURE,\n",
                "        )\n",
                "\n",
                "        features_queue.append(current_feats)\n",
                "        probs_queue.append(current_probs)\n",
                "        if len(features_queue) > MAX_CONTEXT_LENGTH:\n",
                "            features_queue.pop(0)\n",
                "        if len(probs_queue) > MAX_CONTEXT_LENGTH:\n",
                "            probs_queue.pop(0)\n",
                "\n",
                "        current_probs = F.interpolate(\n",
                "            current_probs.movedim(-1, -3)[None, :, :, :],\n",
                "            size=(mask_height, mask_width),\n",
                "            mode=\"nearest\",\n",
                "        )\n",
                "        current_probs = postprocess_probs(current_probs)\n",
                "        current_probs = current_probs.squeeze(0)\n",
                "        mask_probabilities[frame_idx, :, :, :] = current_probs\n",
                "        pred = torch.argmax(current_probs, dim=0).to(dtype=torch.uint8)\n",
                "        mask_predictions[frame_idx, :, :] = pred\n",
                "\n",
                "    torch.cuda.synchronize()\n",
                "    end = time.perf_counter()\n",
                "    print(f\"Processing time: {datetime.timedelta(seconds=round(end - start))}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's visualize a few frames and a video of the result."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if num_frames > 0:\n",
                "    mp.show_images(\n",
                "        [frames[i].convert(\"RGB\") for i in selected_frames]\n",
                "        + [mask_to_rgb(mask_predictions[i], num_masks) for i in selected_frames],\n",
                "        titles=[f\"Frame {i}\" for i in selected_frames] + [\"\"] * len(selected_frames),\n",
                "        columns=len(selected_frames),\n",
                "        height=DISPLAY_HEIGHT,\n",
                "    )\n",
                "\n",
                "    mp.show_videos(\n",
                "        {\n",
                "            \"Input\": [np.array(frame) for frame in frames],\n",
                "            \"Pred\": mask_to_rgb(mask_predictions, num_masks),\n",
                "        },\n",
                "        height=DISPLAY_HEIGHT,\n",
                "        fps=24,\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "This notebook showed how to use DINOv3 for video segmentation tracking. It should be fairly straightforward to run it to your own video and masks. The notebook hyperparameters can also be adjusted to see the effect on the results."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}