{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Segmentation Tracking with DINOv3\n",
                "\n",
                "This notebook demonstrates using DINOv3 for video segmentation tracking using a non-parametric method similar to \"Space-time correspondence as a contrastive random walk\" (Jabri et al. 2020).\n",
                "\n",
                "Given:\n",
                "- RGB video frames\n",
                "- Instance segmentation masks for the first frame\n",
                "\n",
                "We will extract patch features from each frame and use patch similarity to propagate the ground-truth labels to all frames."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup\n",
                "Let's start by loading some pre-requisites, setting up the environment and checking the DINOv3 repository location:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import datetime\n",
                "import functools\n",
                "import io\n",
                "import logging\n",
                "import math\n",
                "import os\n",
                "from pathlib import Path\n",
                "import tarfile\n",
                "import time\n",
                "import urllib\n",
                "\n",
                "import lovely_tensors\n",
                "import matplotlib.pyplot as plt\n",
                "import mediapy as mp\n",
                "import numpy as np\n",
                "from PIL import Image\n",
                "import torch\n",
                "import torch.nn.functional as F\n",
                "import torchvision.transforms as TVT\n",
                "import torchvision.transforms.functional as TVTF\n",
                "from torch import Tensor, nn\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Library Imports\n",
                "from dinov3production.video.tracking import propagate, make_neighborhood_mask\n",
                "\n",
                "DISPLAY_HEIGHT = 200\n",
                "lovely_tensors.monkey_patch()\n",
                "torch.set_grad_enabled(False)\n",
                "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
                "\n",
                "DINOV3_GITHUB_LOCATION = \"facebookresearch/dinov3\"\n",
                "\n",
                "if os.getenv(\"DINOV3_LOCATION\") is not None:\n",
                "    DINOV3_LOCATION = os.getenv(\"DINOV3_LOCATION\")\n",
                "else:\n",
                "    DINOV3_LOCATION = DINOV3_GITHUB_LOCATION\n",
                "\n",
                "print(f\"DINOv3 location set to {DINOV3_LOCATION}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Model\n",
                "We load the DINOv3 ViT-L model and get some attributes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# examples of available DINOv3 models:\n",
                "MODEL_DINOV3_VITS = \"dinov3_vits16\"\n",
                "MODEL_DINOV3_VITSP = \"dinov3_vits16plus\"\n",
                "MODEL_DINOV3_VITB = \"dinov3_vitb16\"\n",
                "MODEL_DINOV3_VITL = \"dinov3_vitl16\"\n",
                "MODEL_DINOV3_VITHP = \"dinov3_vith16plus\"\n",
                "MODEL_DINOV3_VIT7B = \"dinov3_vit7b16\"\n",
                "\n",
                "# we take DINOv3 ViT-L\n",
                "MODEL_NAME = MODEL_DINOV3_VITL\n",
                "\n",
                "model = torch.hub.load(\n",
                "    repo_or_dir=DINOV3_LOCATION,\n",
                "    model=MODEL_NAME,\n",
                "    source=\"local\" if DINOV3_LOCATION != DINOV3_GITHUB_LOCATION else \"github\",\n",
                ")\n",
                "model.to(\"cuda\")\n",
                "model.eval()\n",
                "\n",
                "patch_size = model.patch_size\n",
                "embed_dim = model.embed_dim\n",
                "print(f\"Patch size: {patch_size}\")\n",
                "print(f\"Embedding dimension: {embed_dim}\")\n",
                "print(f\"Peak GPU memory: {torch.cuda.max_memory_allocated() / 2**30:.1f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We want to process one image at the time and get L2-normalized features. Here is a wrapper to do just that."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@torch.compile(disable=True)\n",
                "def forward(\n",
                "    model: nn.Module,\n",
                "    img: Tensor,  # [3, H, W] already normalized for the model\n",
                ") -> Tensor:\n",
                "    feats = model.get_intermediate_layers(img.unsqueeze(0), n=1, reshape=True)[0]  # [1, D, h, w]\n",
                "    feats = feats.movedim(-3, -1)  # [1, h, w, D]\n",
                "    feats = F.normalize(feats, dim=-1, p=2)\n",
                "    return feats.squeeze(0)  # [h, w, D]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data\n",
                "Here we load the video frames and the instance segmentation masks for the first frame."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "VIDEO_FRAMES_URI = \"https://dl.fbaipublicfiles.com/dinov3/notebooks/segmentation_tracking/video_frames.tar.gz\"\n",
                "\n",
                "def load_video_frames_from_remote_tar(tar_uri: str) -> list[Image.Image]:\n",
                "    images = []\n",
                "    indices = []\n",
                "    try:\n",
                "        with urllib.request.urlopen(tar_uri) as f:\n",
                "            tar = tarfile.open(fileobj=io.BytesIO(f.read()))\n",
                "            for member in tar.getmembers():\n",
                "                if member.name.lower().endswith(('.png', '.jpg')):\n",
                "                    index_str, _ = os.path.splitext(os.path.basename(member.name))\n",
                "                    image_data = tar.extractfile(member)\n",
                "                    image = Image.open(image_data).convert(\"RGB\")\n",
                "                    images.append(image)\n",
                "                    indices.append(int(index_str))\n",
                "    except Exception as e:\n",
                "        print(f\"Warning: Failed to load video frames: {e}. Generating dummy frames.\")\n",
                "        # Fallback for offline/test\n",
                "        return [Image.new('RGB', (1920, 1440), color='gray') for _ in range(10)]\n",
                "        \n",
                "    order = np.argsort(indices)\n",
                "    return [images[i] for i in order]\n",
                "\n",
                "frames = load_video_frames_from_remote_tar(VIDEO_FRAMES_URI)\n",
                "num_frames = len(frames)\n",
                "print(f\"Number of frames: {num_frames}\")\n",
                "\n",
                "if num_frames > 0:\n",
                "    original_width, original_height = frames[0].size\n",
                "    print(f\"Original size: width={original_width}, height={original_height}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's show four sample frames from the video:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if num_frames > 0:\n",
                "    num_selected_frames = 4\n",
                "    selected_frames = np.linspace(0, num_frames - 1, num_selected_frames, dtype=int)\n",
                "\n",
                "    mp.show_images(\n",
                "        [frames[int(i)] for i in selected_frames],\n",
                "        titles=[f\"Frame {i}\" for i in selected_frames],\n",
                "        height=DISPLAY_HEIGHT,\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This notebook assumes that instance segmentation masks for the first frame are stored in a .png file:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def mask_to_rgb(mask: np.ndarray | Tensor, num_masks: int) -> np.ndarray:\n",
                "    if isinstance(mask, Tensor):\n",
                "        mask = mask.cpu().numpy()\n",
                "\n",
                "    # Exclude background\n",
                "    background = mask == 0\n",
                "    mask = mask - 1\n",
                "    num_masks = num_masks - 1\n",
                "\n",
                "    # Choose palette\n",
                "    if num_masks <= 10:\n",
                "        mask_rgb = plt.get_cmap(\"tab10\")(mask)[..., :3]\n",
                "    elif num_masks <= 20:\n",
                "        mask_rgb = plt.get_cmap(\"tab20\")(mask)[..., :3]\n",
                "    else:\n",
                "        mask_rgb = plt.get_cmap(\"gist_rainbow\")(mask / (num_masks - 1))[..., :3]\n",
                "\n",
                "    mask_rgb = (mask_rgb * 255).astype(np.uint8)\n",
                "    mask_rgb[background, :] = 0\n",
                "    return mask_rgb\n",
                "\n",
                "\n",
                "def load_image_from_url(url: str) -> Image:\n",
                "    try:\n",
                "        with urllib.request.urlopen(url) as f:\n",
                "            return Image.open(f)\n",
                "    except:\n",
                "        return Image.new('L', (1920, 1440), color=0)\n",
                "\n",
                "\n",
                "first_mask_np = np.array(\n",
                "    load_image_from_url(\n",
                "        \"https://dl.fbaipublicfiles.com/dinov3/notebooks/segmentation_tracking/first_video_frame_mask.png\"\n",
                "    )\n",
                ")\n",
                "\n",
                "if first_mask_np.max() == 0 and num_frames > 0:\n",
                "     # Dummy mask if load failed\n",
                "     first_mask_np[100:500, 100:500] = 1\n",
                "     first_mask_np[600:900, 600:900] = 2\n",
                "\n",
                "mask_height, mask_width = first_mask_np.shape \n",
                "print(f\"Mask size: {[mask_height, mask_width]}\")\n",
                "\n",
                "num_masks = int(first_mask_np.max() + 1)\n",
                "print(f\"Number of masks: {num_masks}\")\n",
                "\n",
                "if num_frames > 0:\n",
                "    mp.show_images(\n",
                "        [frames[0], mask_to_rgb(first_mask_np, num_masks)],\n",
                "        titles=[\"Frame\", \"Mask\"],\n",
                "        height=DISPLAY_HEIGHT,\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Transforms\n",
                "Input frames need to be resized to match the desired forward resolution and the model patch size."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ResizeToMultiple(nn.Module):\n",
                "    def __init__(self, short_side: int, multiple: int):\n",
                "        super().__init__()\n",
                "        self.short_side = short_side\n",
                "        self.multiple = multiple\n",
                "\n",
                "    def _round_up(self, side: float) -> int:\n",
                "        return math.ceil(side / self.multiple) * self.multiple\n",
                "\n",
                "    def forward(self, img):\n",
                "        old_width, old_height = TVTF.get_image_size(img)\n",
                "        if old_width > old_height:\n",
                "            new_height = self._round_up(self.short_side)\n",
                "            new_width = self._round_up(old_width * new_height / old_height)\n",
                "        else:\n",
                "            new_width = self._round_up(self.short_side)\n",
                "            new_height = self._round_up(old_height * new_width / old_width)\n",
                "        return TVTF.resize(img, [new_height, new_width], interpolation=TVT.InterpolationMode.BICUBIC)\n",
                "\n",
                "\n",
                "SHORT_SIDE = 960\n",
                "\n",
                "transform = TVT.Compose(\n",
                "    [\n",
                "        ResizeToMultiple(short_side=SHORT_SIDE, multiple=patch_size),\n",
                "        TVT.ToTensor(),\n",
                "        TVT.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "    ]\n",
                ")\n",
                "if num_frames > 0:\n",
                "    first_frame = transform(frames[0]).to(\"cuda\")\n",
                "    print(f\"First frame: {first_frame}\")\n",
                "\n",
                "    _, frame_height, frame_width = first_frame.shape\n",
                "    feats_height, feats_width = frame_height // patch_size, frame_width // patch_size"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Label propagation happens at the output resolution of the model, so we downsample the ground-truth masks of the first frame and turn them into a one-hot probability map."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "first_mask = torch.from_numpy(first_mask_np).to(\"cuda\", dtype=torch.long)\n",
                "first_mask = F.interpolate(\n",
                "    first_mask[None, None, :, :].float(),\n",
                "    (feats_height, feats_width),\n",
                "    mode=\"nearest-exact\",\n",
                ")[0, 0].long()\n",
                "\n",
                "first_probs = F.one_hot(first_mask, num_masks).float()\n",
                "print(f\"First mask shape: {first_mask.shape}\")\n",
                "print(f\"First probs shape: {first_probs.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## How it works\n",
                "Label propagation takes as input current features, context features, and probabilities, and computes similarity."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Using library implementations for optimized propagation\n",
                "from dinov3production.video.tracking import propagate, make_neighborhood_mask"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Visualization of neighborhood mask:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "neighborhood_mask = make_neighborhood_mask(feats_height, feats_width, size=12, shape=\"circle\")\n",
                "\n",
                "mp.show_images(\n",
                "    {f\"{(i, j)}\": neighborhood_mask[i, j].cpu().numpy() for i, j in [[3, 14], [20, 25]]},\n",
                "    height=DISPLAY_HEIGHT,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To understand how it works, let's do it for one frame only. The \"context\" contains only the first frame and the \"current frame\" is the second one."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if num_frames > 1:\n",
                "    torch._dynamo.maybe_mark_dynamic(first_frame, (1, 2))\n",
                "    first_feats = forward(model, first_frame)\n",
                "\n",
                "    frame_idx = 1\n",
                "    current_frame_pil = frames[frame_idx]\n",
                "    current_frame = transform(current_frame_pil).to(\"cuda\")\n",
                "    torch._dynamo.maybe_mark_dynamic(current_frame, (1, 2))\n",
                "    current_feats = forward(model, current_frame)\n",
                "\n",
                "    current_probs = propagate(\n",
                "        current_feats,\n",
                "        context_features=first_feats.unsqueeze(0),\n",
                "        context_probs=first_probs.unsqueeze(0),\n",
                "        neighborhood_mask=neighborhood_mask,\n",
                "        topk=5,\n",
                "        temperature=0.2,\n",
                "    )\n",
                "    print(f\"Current probs shape: {current_probs.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Then, we upsample the predicted probabilities and postprocess them."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def postprocess_probs(probs: Tensor) -> Tensor:\n",
                "    vmin = probs.flatten(2, 3).min(dim=2).values\n",
                "    vmax = probs.flatten(2, 3).max(dim=2).values\n",
                "    probs = (probs - vmin[:, :, None, None]) / (vmax[:, :, None, None] - vmin[:, :, None, None])\n",
                "    probs = torch.nan_to_num(probs, nan=0)\n",
                "    return probs\n",
                "\n",
                "if num_frames > 1:\n",
                "    p = current_probs.movedim(-1, -3).unsqueeze(0)\n",
                "    p = F.interpolate(p, size=(mask_height, mask_width), mode=\"nearest\")\n",
                "    p = postprocess_probs(p).squeeze(0)\n",
                "    current_pred_np = p.argmax(0).cpu().numpy()\n",
                "    current_probs_np = p.cpu().numpy()\n",
                "\n",
                "    mp.show_images(\n",
                "        [\n",
                "            frames[0],\n",
                "            current_frame_pil,\n",
                "            mask_to_rgb(first_mask_np, num_masks),\n",
                "            mask_to_rgb(current_pred_np, num_masks),\n",
                "        ],\n",
                "        titles=[\"First frame\", \"Second frame\", \"\", \"\"],\n",
                "        columns=2,\n",
                "        height=DISPLAY_HEIGHT,\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Process Video\n",
                "Process all frames with context queue."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "MAX_CONTEXT_LENGTH = 7\n",
                "NEIGHBORHOOD_SIZE = 12\n",
                "NEIGHBORHOOD_SHAPE = \"circle\"\n",
                "TOPK = 5\n",
                "TEMPERATURE = 0.2\n",
                "\n",
                "mask_predictions = torch.zeros([num_frames, mask_height, mask_width], dtype=torch.uint8)\n",
                "mask_predictions[0, :, :] = torch.from_numpy(first_mask_np)\n",
                "\n",
                "mask_probabilities = torch.zeros([num_frames, num_masks, mask_height, mask_width])\n",
                "mask_probabilities[0, :, :, :] = F.one_hot(torch.from_numpy(first_mask_np).long(), num_masks).movedim(-1, -3)\n",
                "\n",
                "features_queue: list[Tensor] = []\n",
                "probs_queue: list[Tensor] = []\n",
                "\n",
                "neighborhood_mask = make_neighborhood_mask(\n",
                "    feats_height,\n",
                "    feats_width,\n",
                "    size=NEIGHBORHOOD_SIZE,\n",
                "    shape=NEIGHBORHOOD_SHAPE,\n",
                ")\n",
                "\n",
                "if num_frames > 1:\n",
                "    start = time.perf_counter()\n",
                "    for frame_idx in tqdm(range(1, num_frames), desc=\"Processing\"):\n",
                "        current_frame_pil = frames[frame_idx]\n",
                "        current_frame = transform(current_frame_pil).to(\"cuda\")\n",
                "        torch._dynamo.maybe_mark_dynamic(current_frame, (1, 2))\n",
                "        current_feats = forward(model, current_frame)\n",
                "\n",
                "        context_feats = torch.stack([first_feats, *features_queue], dim=0)\n",
                "        context_probs = torch.stack([first_probs, *probs_queue], dim=0)\n",
                "        torch._dynamo.maybe_mark_dynamic(context_feats, 0)\n",
                "        torch._dynamo.maybe_mark_dynamic(context_probs, (0, 3))\n",
                "\n",
                "        current_probs = propagate(\n",
                "            current_feats,\n",
                "            context_feats,\n",
                "            context_probs,\n",
                "            neighborhood_mask,\n",
                "            TOPK,\n",
                "            TEMPERATURE,\n",
                "        )\n",
                "\n",
                "        features_queue.append(current_feats)\n",
                "        probs_queue.append(current_probs)\n",
                "        if len(features_queue) > MAX_CONTEXT_LENGTH:\n",
                "            features_queue.pop(0)\n",
                "        if len(probs_queue) > MAX_CONTEXT_LENGTH:\n",
                "            probs_queue.pop(0)\n",
                "\n",
                "        current_probs = F.interpolate(\n",
                "            current_probs.movedim(-1, -3)[None, :, :, :],\n",
                "            size=(mask_height, mask_width),\n",
                "            mode=\"nearest\",\n",
                "        )\n",
                "        current_probs = postprocess_probs(current_probs)\n",
                "        current_probs = current_probs.squeeze(0)\n",
                "        mask_probabilities[frame_idx, :, :, :] = current_probs\n",
                "        pred = torch.argmax(current_probs, dim=0).to(dtype=torch.uint8)\n",
                "        mask_predictions[frame_idx, :, :] = pred\n",
                "\n",
                "    torch.cuda.synchronize()\n",
                "    end = time.perf_counter()\n",
                "    print(f\"Processing time: {datetime.timedelta(seconds=round(end - start))}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's visualize a few frames and a video of the result."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if num_frames > 0:\n",
                "    mp.show_images(\n",
                "        [frames[i].convert(\"RGB\") for i in selected_frames]\n",
                "        + [mask_to_rgb(mask_predictions[i], num_masks) for i in selected_frames],\n",
                "        titles=[f\"Frame {i}\" for i in selected_frames] + [\"\"] * len(selected_frames),\n",
                "        columns=len(selected_frames),\n",
                "        height=DISPLAY_HEIGHT,\n",
                "    )\n",
                "\n",
                "    mp.show_videos(\n",
                "        {\n",
                "            \"Input\": [np.array(frame) for frame in frames],\n",
                "            \"Pred\": mask_to_rgb(mask_predictions, num_masks),\n",
                "        },\n",
                "        height=DISPLAY_HEIGHT,\n",
                "        fps=24,\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "This notebook showed how to use DINOv3 for video segmentation tracking. It should be fairly straightforward to run it to your own video and masks. The notebook hyperparameters can also be adjusted to see the effect on the results."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}